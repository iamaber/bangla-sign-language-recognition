{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unified BDSL-SPOTER Training Notebook\n",
        "\n",
        "**Overview**: This notebook consolidates all scattered code from the BDSLW_SPOTER project into a single, production-ready training pipeline with WandB tracking.\n",
        "\n",
        "**Dataset**: Bengali Sign Language (BdSL) with 72 words, 833 videos from 3 signers (S01, S02, S05)\n",
        "\n",
        "**Model**: ProductionSPOTER - Transformer-based architecture optimized for sign language recognition\n",
        "\n",
        "**Features**:\n",
        "- Full WandB integration for experiment tracking\n",
        "- Proper train/val/test splits (665/82/86)\n",
        "- 108D pose features (33 landmarks √ó 3 coords + padding)\n",
        "- 150 frames max sequence length\n",
        "- Data augmentation (temporal, spatial, perspective)\n",
        "- Mixed precision training\n",
        "- Early stopping with checkpointing\n",
        "- Comprehensive evaluation (per-signer, per-class)\n",
        "\n",
        "**Configuration**:\n",
        "- Epochs: 50\n",
        "- Batch size: 16\n",
        "- Learning rate: 3e-4\n",
        "- Primary metric: Top-1 accuracy\n",
        "- Classes: 72 Bengali words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration loaded\n",
            "   Total samples: 833\n",
            "   Number of classes: 72\n",
            "   Epochs: 50\n",
            "   Batch size: 16\n",
            "   Normalized dir: /home/raco/Repos/bangla-sign-language-recognition/Data/processed/new_model/normalized\n"
          ]
        }
      ],
      "source": [
        "CONFIG = {\n",
        "    # Dataset\n",
        "    \"total_samples\": 833,\n",
        "    \"num_classes\": 72,\n",
        "    \"train_val_test\": \"665/82/86\",\n",
        "\n",
        "    # Model\n",
        "    \"input_dim\": 108,\n",
        "    \"seq_length\": 150,\n",
        "    \"num_encoder_layers\": 8,\n",
        "    \"num_heads\": 9,\n",
        "    \"d_model\": 108,\n",
        "    \"d_ff\": 512,\n",
        "\n",
        "    # Training\n",
        "    \"epochs\": 50,\n",
        "    \"batch_size\": 16,\n",
        "    \"learning_rate\": 3e-4,\n",
        "    \"weight_decay\": 0.05,\n",
        "    \"label_smoothing\": 0.1,\n",
        "    \"dropout\": 0.15,\n",
        "    \"early_stopping_patience\": 15,\n",
        "    \"primary_metric\": \"top-1_accuracy\",\n",
        "    \"augmentation\": True,\n",
        "\n",
        "    # Preprocessing\n",
        "    \"max_frames\": 300,\n",
        "    \"mediapipe_complexity\": 2,\n",
        "\n",
        "    # Paths\n",
        "    \"base_dir\": \"/home/raco/Repos/bangla-sign-language-recognition\",\n",
        "    \"processed_dir\": \"Data/processed/new_model\",\n",
        "    \"normalized_dir\": \"/home/raco/Repos/bangla-sign-language-recognition/Data/processed/new_model/normalized\",\n",
        "    \"checkpoint_dir\": \"/home/raco/Repos/bangla-sign-language-recognition/Data/processed/new_model/checkpoints\",\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Configuration loaded\")\n",
        "print(f\"   Total samples: {CONFIG['total_samples']}\")\n",
        "print(f\"   Number of classes: {CONFIG['num_classes']}\")\n",
        "print(f\"   Epochs: {CONFIG['epochs']}\")\n",
        "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"   Normalized dir: {CONFIG['normalized_dir']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Device & Reproducibility Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Device: cuda\n",
            "‚úÖ GPU: NVIDIA GeForce RTX 3090 Ti\n",
            "‚úÖ CUDA Version: 12.8\n",
            "‚úÖ GPU Memory: 23.54 GB\n",
            "‚úÖ Random seed set to 50\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"‚úÖ Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 50\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "print(f\"‚úÖ Random seed set to {SEED}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2.5: Warning Suppression & MediaPipe Setup\n",
        "**Note**: Warning suppression to reduce MediaPipe/TF Lite verbosity\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress MediaPipe and TensorFlow Lite warnings\n",
        "import os\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "# Suppress TensorFlow Lite warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TF C++ logs\n",
        "os.environ['GLOG_minloglevel'] = '2'  # Suppress Google logs\n",
        "\n",
        "# Suppress Python warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', message='Feedback manager requires')\n",
        "\n",
        "# Suppress MediaPipe specific warnings\n",
        "logging.getLogger('mediapipe').setLevel(logging.ERROR)\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "\n",
        "print(\"‚úÖ Warnings suppressed\")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ MediaPipe already installed\n",
            "üì• Downloading Pose Landmark model...\n",
            "  ‚úÖ Using cached model: /home/raco/.cache/mediapipe/models/pose_landmarker_heavy.task\n",
            "‚úÖ MediaPipe PoseLandmarker initialized (new API)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "W0000 00:00:1769673564.917717  205223 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1769673564.967393  205234 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "try:\n",
        "    import mediapipe as mp\n",
        "    print(\"‚úÖ MediaPipe already installed\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"üì¶ Installing MediaPipe (this may take a minute)...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"--quiet\", \"install\", \"mediapipe>=0.10.0\"])\n",
        "    import mediapipe as mp\n",
        "    print(\"‚úÖ MediaPipe installed successfully\")\n",
        "\n",
        "# New MediaPipe API (0.10.x)\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "# Download pose landmark model (required for new API)\n",
        "print(\"üì• Downloading Pose Landmark model...\")\n",
        "MODEL_URL = \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\"\n",
        "cache_dir = Path.home() / \".cache\" / \"mediapipe\" / \"models\"\n",
        "cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "model_path = cache_dir / \"pose_landmarker_heavy.task\"\n",
        "\n",
        "if not model_path.exists():\n",
        "    print(f\"  Downloading to: {model_path}\")\n",
        "    try:\n",
        "        urllib.request.urlretrieve(MODEL_URL, model_path)\n",
        "        print(\"  ‚úÖ Model downloaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Download failed: {e}\")\n",
        "        # Try alternative URL\n",
        "        MODEL_URL_ALT = \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task\"\n",
        "        model_path = cache_dir / \"pose_landmarker_lite.task\"\n",
        "        print(f\"  Trying lightweight model...\")\n",
        "        urllib.request.urlretrieve(MODEL_URL_ALT, model_path)\n",
        "        print(\"  ‚úÖ Lightweight model downloaded\")\n",
        "else:\n",
        "    print(f\"  ‚úÖ Using cached model: {model_path}\")\n",
        "\n",
        "# Create PoseLandmarker with downloaded model\n",
        "base_options = python.BaseOptions(model_asset_path=str(model_path))\n",
        "options = vision.PoseLandmarkerOptions(\n",
        "    base_options=base_options,\n",
        "    output_segmentation_masks=False,\n",
        "    num_poses=1\n",
        ")\n",
        "pose_landmarker = vision.PoseLandmarker.create_from_options(options)\n",
        "\n",
        "# Define landmarks (use new API\\'s PoseLandmark enum)\n",
        "LEFT_SHOULDER = vision.PoseLandmark.LEFT_SHOULDER.value\n",
        "RIGHT_SHOULDER = vision.PoseLandmark.RIGHT_SHOULDER.value\n",
        "MIN_SHOULDER_SCALE = 0.1\n",
        "\n",
        "print(\"‚úÖ MediaPipe PoseLandmarker initialized (new API)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2.6: Landmark Extraction Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_or_crop_centered(sequence, target_length):\n",
        "    length = sequence.shape[0]\n",
        "    if length == target_length:\n",
        "        return sequence\n",
        "    if length > target_length:\n",
        "        start = max(0, (length - target_length) // 2)\n",
        "        return sequence[start:start + target_length]\n",
        "    pad_shape = (target_length - length,) + sequence.shape[1:]\n",
        "    padding = np.zeros(pad_shape, dtype=sequence.dtype)\n",
        "    return np.concatenate([sequence, padding], axis=0)\n",
        "\n",
        "\n",
        "def normalize_pose_sequence(pose_sequence):\n",
        "    left = pose_sequence[:, LEFT_SHOULDER, :3]\n",
        "    right = pose_sequence[:, RIGHT_SHOULDER, :3]\n",
        "    neck = (left + right) / 2.0\n",
        "    shoulder_width = np.linalg.norm(left - right, axis=-1)\n",
        "    valid = np.isfinite(shoulder_width) & (shoulder_width > 0)\n",
        "    scale = float(shoulder_width[valid].mean()) if np.any(valid) else MIN_SHOULDER_SCALE\n",
        "    scale = max(scale, MIN_SHOULDER_SCALE)\n",
        "    pose_centered = pose_sequence - neck[:, None, :]\n",
        "    pose_normalized = pose_centered / scale\n",
        "    return pose_normalized\n",
        "\n",
        "\n",
        "def extract_pose_sequence(video_path, seq_length=CONFIG['seq_length'], max_frames=CONFIG['max_frames']):\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise FileNotFoundError(f\"Video not found: {video_path}\")\n",
        "    frames = []\n",
        "    \n",
        "    # Process frames with new MediaPipe API\n",
        "    while True:\n",
        "        success, frame = cap.read()\n",
        "        if not success or len(frames) >= max_frames:\n",
        "            break\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Create MediaPipe Image and detect pose\n",
        "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
        "        detection_result = pose_landmarker.detect(mp_image)\n",
        "        \n",
        "        if detection_result.pose_landmarks and len(detection_result.pose_landmarks) > 0:\n",
        "            # Extract 33 pose landmarks (x, y, z)\n",
        "            coords = np.array([[lm.x, lm.y, lm.z] for lm in detection_result.pose_landmarks[0]], dtype=np.float32)\n",
        "        else:\n",
        "            coords = np.zeros((33, 3), dtype=np.float32)\n",
        "        frames.append(coords)\n",
        "    \n",
        "    cap.release()\n",
        "    if not frames:\n",
        "        raise RuntimeError(f\"No frames decoded from {video_path}\")\n",
        "    pose_sequence = np.stack(frames)\n",
        "    normalized = normalize_pose_sequence(pose_sequence)\n",
        "    normalized = pad_or_crop_centered(normalized, seq_length)\n",
        "    features = normalized.reshape(seq_length, -1)\n",
        "    if features.shape[1] < CONFIG['input_dim']:\n",
        "        padding = np.zeros((seq_length, CONFIG['input_dim'] - features.shape[1]), dtype=np.float32)\n",
        "        features = np.hstack([features, padding])\n",
        "    elif features.shape[1] > CONFIG['input_dim']:\n",
        "        features = features[:, :CONFIG['input_dim']]\n",
        "    return features.astype(np.float32), pose_sequence.shape[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Load Sample Lists (Train/Val/Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_sample_list(file_path):\n",
        "    \"\"\"Load sample paths from text file\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        samples = [line.strip() for line in f if line.strip()]\n",
        "    return samples\n",
        "\n",
        "base_path = Path(CONFIG['base_dir'])\n",
        "train_samples = load_sample_list(base_path / CONFIG['processed_dir'] / 'train_samples.txt')\n",
        "val_samples = load_sample_list(base_path / CONFIG['processed_dir'] / 'val_samples.txt')\n",
        "test_samples = load_sample_list(base_path / CONFIG['processed_dir'] / 'test_samples.txt')\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(train_samples)} train samples\")\n",
        "print(f\"‚úÖ Loaded {len(val_samples)} val samples\")\n",
        "print(f\"‚úÖ Loaded {len(test_samples)} test samples\")\n",
        "print(f\"   Total: {len(train_samples) + len(val_samples) + len(test_samples)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Parse Video Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_video_metadata(video_path):\n",
        "    \"\"\"\n",
        "    Extract metadata from video filename\n",
        "    Format: <word>__S<XX>__sess<YY>__rep<ZZ>__<grammar>.mp4\n",
        "    Example: ‡¶Ü‡¶Æ‡¶ø__S01__sess01__rep01__neutral.mp4\n",
        "    \"\"\"\n",
        "    filename = Path(video_path).stem\n",
        "    parts = filename.split('__')\n",
        "    \n",
        "    if len(parts) != 5:\n",
        "        print(f\"Warning: Unexpected filename format: {filename}\")\n",
        "        return None\n",
        "    \n",
        "    word = parts[0]\n",
        "    signer = parts[1]\n",
        "    session = parts[2]\n",
        "    repetition = parts[3]\n",
        "    grammar = parts[4]\n",
        "    \n",
        "    return {\n",
        "        'word': word,\n",
        "        'signer': signer,\n",
        "        'session': session,\n",
        "        'repetition': repetition,\n",
        "        'grammar': grammar,\n",
        "        'full_path': video_path\n",
        "    }\n",
        "\n",
        "# Parse all samples\n",
        "train_metadata = [parse_video_metadata(s) for s in train_samples]\n",
        "val_metadata = [parse_video_metadata(s) for s in val_samples]\n",
        "test_metadata = [parse_video_metadata(s) for s in test_samples]\n",
        "\n",
        "# Filter out None values (if any)\n",
        "train_metadata = [m for m in train_metadata if m is not None]\n",
        "val_metadata = [m for m in val_metadata if m is not None]\n",
        "test_metadata = [m for m in test_metadata if m is not None]\n",
        "\n",
        "print(f\"‚úÖ Parsed metadata for {len(train_metadata)} train samples\")\n",
        "print(f\"‚úÖ Parsed metadata for {len(val_metadata)} val samples\")\n",
        "print(f\"‚úÖ Parsed metadata for {len(test_metadata)} test samples\")\n",
        "\n",
        "# Show sample\n",
        "if train_metadata:\n",
        "    print(f\"\\nSample metadata:\")\n",
        "    for key, value in train_metadata[0].items():\n",
        "        print(f\"   {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4.5: Pose Extraction & .npz Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "\n",
        "normalized_dir = Path(CONFIG['normalized_dir'])\n",
        "normalized_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"‚úÖ Normalized data directory: {normalized_dir}\")\n",
        "\n",
        "\n",
        "def build_npz_path(metadata):\n",
        "    filename = f\"{metadata['word']}__{metadata['signer']}__{metadata['session']}__{metadata['repetition']}__{metadata['grammar']}.npz\"\n",
        "    return normalized_dir / filename\n",
        "\n",
        "\n",
        "def preprocess_split(split_name, sample_paths):\n",
        "    stats = {\"created\": 0, \"skipped\": 0, \"failed\": 0}\n",
        "    failures = []\n",
        "    for sample in tqdm(sample_paths, desc=f\"Preprocessing {split_name}\", leave=False):\n",
        "        metadata = parse_video_metadata(sample)\n",
        "        if metadata is None:\n",
        "            stats[\"failed\"] += 1\n",
        "            failures.append((sample, \"invalid filename\"))\n",
        "            continue\n",
        "        npz_path = build_npz_path(metadata)\n",
        "        if npz_path.exists():\n",
        "            stats[\"skipped\"] += 1\n",
        "            continue\n",
        "        video_path = base_path / sample\n",
        "        try:\n",
        "            features, raw_length = extract_pose_sequence(video_path)\n",
        "            np.savez_compressed(npz_path, pose_sequence=features, raw_length=raw_length)\n",
        "            stats[\"created\"] += 1\n",
        "        except Exception as exc:\n",
        "            stats[\"failed\"] += 1\n",
        "            failures.append((sample, str(exc)))\n",
        "    return stats, failures\n",
        "\n",
        "start_time = perf_counter()\n",
        "split_stats = {}\n",
        "failure_log = {}\n",
        "for split_name, samples in [(\"Train\", train_samples), (\"Val\", val_samples), (\"Test\", test_samples)]:\n",
        "    stats, failures = preprocess_split(split_name, samples)\n",
        "    split_stats[split_name] = stats\n",
        "    if failures:\n",
        "        failure_log[split_name] = failures\n",
        "elapsed = perf_counter() - start_time\n",
        "\n",
        "print(\"üì¶ Preprocessing summary:\")\n",
        "for split_name, stats in split_stats.items():\n",
        "    print(f\"   {split_name}: created {stats['created']}, skipped {stats['skipped']}, failed {stats['failed']}\")\n",
        "print(f\"   Elapsed time: {elapsed/60:.2f} min\")\n",
        "print(f\"   Total .npz files: {len(list(normalized_dir.glob('*.npz')))}\")\n",
        "\n",
        "if failure_log:\n",
        "    print(\"‚ö†Ô∏è  Failed samples (first 5 per split):\")\n",
        "    for split_name, failures in failure_log.items():\n",
        "        print(f\"{split_name}:\")\n",
        "        for sample, error in failures[:5]:\n",
        "            print(f\" - {sample}: {error}\")\n",
        "else:\n",
        "    print(\"‚úÖ All videos processed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Build Word-to-Label Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Extract unique words from all samples\n",
        "all_metadata = train_metadata + val_metadata + test_metadata\n",
        "all_words = sorted(set([m['word'] for m in all_metadata]))\n",
        "\n",
        "print(f\"‚úÖ Found {len(all_words)} unique Bengali words\")\n",
        "print(f\"   First 10: {all_words[:10]}\")\n",
        "print(f\"   Last 10: {all_words[-10:]}\")\n",
        "\n",
        "# Create mappings\n",
        "word_to_label = {word: idx for idx, word in enumerate(all_words)}\n",
        "label_to_word = {idx: word for idx, word in enumerate(all_words)}\n",
        "\n",
        "print(\"‚úÖ Created word-to-label mapping\")\n",
        "print(f\"   Example: '‡¶Ü‡¶Æ‡¶ø' -> {word_to_label['‡¶Ü‡¶Æ‡¶ø']}\")\n",
        "print(f\"   Example: {label_to_word[0]} <- 0\")\n",
        "\n",
        "# Save mappings\n",
        "checkpoint_dir = Path(CONFIG['checkpoint_dir'])\n",
        "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(checkpoint_dir / 'label_mapping.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump({'word_to_label': word_to_label, 'label_to_word': label_to_word}, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Label mappings saved to {checkpoint_dir / 'label_mapping.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Data Analysis & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataFrames for analysis\n",
        "train_df = pd.DataFrame(train_metadata)\n",
        "val_df = pd.DataFrame(val_metadata)\n",
        "test_df = pd.DataFrame(test_metadata)\n",
        "\n",
        "# Signer distribution\n",
        "print(\"\\nüìä Signer Distribution:\")\n",
        "print(\"=\"*50)\n",
        "for split_name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
        "    print(f\"\\n{split_name} Set:\")\n",
        "    for signer in ['S01', 'S02', 'S05']:\n",
        "        count = len(df[df['signer'] == signer])\n",
        "        percentage = count / len(df) * 100 if len(df) > 0 else 0\n",
        "        print(f\"   {signer}: {count:3d} samples ({percentage:5.1f}%)\")\n",
        "    print(f\"   Total: {len(df)} samples\")\n",
        "\n",
        "# Grammar distribution\n",
        "print(\"\\n\\nüìä Grammar/Emotion Distribution:\")\n",
        "print(\"=\"*50)\n",
        "for split_name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
        "    print(f\"\\n{split_name} Set:\")\n",
        "    grammar_counts = df['grammar'].value_counts()\n",
        "    for grammar in ['neutral', 'question', 'negation', 'happy', 'sad']:\n",
        "        count = grammar_counts.get(grammar, 0)\n",
        "        percentage = count / len(df) * 100 if len(df) > 0 else 0\n",
        "        print(f\"   {grammar:10s}: {count:3d} samples ({percentage:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Signer distribution\n",
        "for idx, (split_name, df, color) in enumerate([\n",
        "    ('Train', train_df, 'skyblue'),\n",
        "    ('Val', val_df, 'lightgreen'),\n",
        "    ('Test', test_df, 'salmon')\n",
        "]):\n",
        "    signer_counts = df['signer'].value_counts()\n",
        "    axes[idx].bar(signer_counts.index, signer_counts.values, color=color, alpha=0.7)\n",
        "    axes[idx].set_title(f'{split_name} Set - Signer Distribution')\n",
        "    axes[idx].set_xlabel('Signer')\n",
        "    axes[idx].set_ylabel('Number of Samples')\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(checkpoint_dir / 'signer_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Saved signer distribution plot to {checkpoint_dir / 'signer_distribution.png'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grammar distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, (split_name, df, color) in enumerate([\n",
        "    ('Train', train_df, 'skyblue'),\n",
        "    ('Val', val_df, 'lightgreen'),\n",
        "    ('Test', test_df, 'salmon')\n",
        "]):\n",
        "    grammar_counts = df['grammar'].value_counts()\n",
        "    axes[idx].bar(grammar_counts.index, grammar_counts.values, color=color, alpha=0.7)\n",
        "    axes[idx].set_title(f'{split_name} Set - Grammar Distribution')\n",
        "    axes[idx].set_xlabel('Grammar/Emotion')\n",
        "    axes[idx].set_ylabel('Number of Samples')\n",
        "    axes[idx].tick_params(axis='x', rotation=45)\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(checkpoint_dir / 'grammar_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Saved grammar distribution plot to {checkpoint_dir / 'grammar_distribution.png'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: BdSLDataset Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class BdSLDataset(Dataset):\n",
        "    def __init__(self, sample_paths, word_to_label, normalized_dir,\n",
        "                 max_seq_length=150, augment=False, mode='train'):\n",
        "        self.sample_paths = sample_paths\n",
        "        self.word_to_label = word_to_label\n",
        "        self.normalized_dir = Path(normalized_dir)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.augment = augment and (mode == 'train')\n",
        "        self.mode = mode\n",
        "\n",
        "        self.metadata_list = [parse_video_metadata(s) for s in sample_paths]\n",
        "        self.metadata_list = [m for m in self.metadata_list if m is not None]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata_list)\n",
        "\n",
        "    def _npz_path(self, metadata):\n",
        "        filename = f\"{metadata['word']}__{metadata['signer']}__{metadata['session']}__{metadata['repetition']}__{metadata['grammar']}.npz\"\n",
        "        return self.normalized_dir / filename\n",
        "\n",
        "    def _load_pose_sequence(self, metadata):\n",
        "        npz_path = self._npz_path(metadata)\n",
        "        if not npz_path.exists():\n",
        "            raise FileNotFoundError(f\"Missing .npz: {npz_path.name}\")\n",
        "        data = np.load(npz_path)\n",
        "        if 'pose_sequence' in data:\n",
        "            pose_sequence = data['pose_sequence']\n",
        "        else:\n",
        "            keys = list(data.keys())\n",
        "            if not keys:\n",
        "                raise ValueError(f\"Empty .npz: {npz_path.name}\")\n",
        "            pose_sequence = data[keys[0]]\n",
        "            if pose_sequence.ndim == 3:\n",
        "                pose_sequence = pose_sequence.reshape(pose_sequence.shape[0], -1)\n",
        "        return np.asarray(pose_sequence, dtype=np.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        metadata = self.metadata_list[idx]\n",
        "        label = self.word_to_label[metadata['word']]\n",
        "\n",
        "        try:\n",
        "            pose_sequence = self._load_pose_sequence(metadata)\n",
        "            seq_length = min(len(pose_sequence), self.max_seq_length)\n",
        "        except Exception as exc:\n",
        "            print(f\"‚ö†Ô∏è  {exc}\")\n",
        "            pose_sequence = np.zeros((self.max_seq_length, CONFIG['input_dim']), dtype=np.float32)\n",
        "            seq_length = 0\n",
        "\n",
        "        if pose_sequence.ndim == 3:\n",
        "            pose_sequence = pose_sequence.reshape(pose_sequence.shape[0], -1)\n",
        "\n",
        "        if pose_sequence.shape[1] < CONFIG['input_dim']:\n",
        "            padding = np.zeros((pose_sequence.shape[0], CONFIG['input_dim'] - pose_sequence.shape[1]), dtype=np.float32)\n",
        "            pose_sequence = np.hstack([pose_sequence, padding])\n",
        "        elif pose_sequence.shape[1] > CONFIG['input_dim']:\n",
        "            pose_sequence = pose_sequence[:, :CONFIG['input_dim']]\n",
        "\n",
        "        pose_sequence = pad_or_crop_centered(pose_sequence, self.max_seq_length).astype(np.float32)\n",
        "\n",
        "        if self.augment:\n",
        "            pose_sequence = self._augment_pose_sequence(pose_sequence)\n",
        "\n",
        "        attention_mask = np.zeros(self.max_seq_length, dtype=np.float32)\n",
        "        attention_mask[:seq_length] = 1\n",
        "\n",
        "        return {\n",
        "            'pose_sequence': torch.FloatTensor(pose_sequence),\n",
        "            'label': torch.LongTensor([label]),\n",
        "            'attention_mask': torch.FloatTensor(attention_mask),\n",
        "            'seq_length': torch.LongTensor([seq_length]),\n",
        "            'word': metadata['word'],\n",
        "            'signer': metadata['signer'],\n",
        "            'grammar': metadata['grammar']\n",
        "        }\n",
        "\n",
        "    def _augment_pose_sequence(self, sequence):\n",
        "        \"\"\"Apply data augmentation\"\"\"\n",
        "        if np.random.random() > 0.7:\n",
        "            return sequence\n",
        "\n",
        "        augmented = sequence.copy()\n",
        "\n",
        "        if np.random.random() < 0.3:\n",
        "            scale_factor = np.random.uniform(0.8, 1.2)\n",
        "            new_length = max(1, int(len(augmented) * scale_factor))\n",
        "            indices = np.linspace(0, len(augmented) - 1, new_length)\n",
        "            resampled = []\n",
        "            for idx in indices:\n",
        "                lower, upper = int(np.floor(idx)), min(int(np.ceil(idx)), len(augmented) - 1)\n",
        "                weight = idx - lower\n",
        "                resampled.append((1 - weight) * augmented[lower] + weight * augmented[upper])\n",
        "            augmented = np.array(resampled)\n",
        "\n",
        "            if len(augmented) < self.max_seq_length:\n",
        "                padding = np.zeros((self.max_seq_length - len(augmented), CONFIG['input_dim']))\n",
        "                augmented = np.vstack([augmented, padding])\n",
        "            elif len(augmented) > self.max_seq_length:\n",
        "                augmented = augmented[:self.max_seq_length]\n",
        "\n",
        "        if np.random.random() < 0.4:\n",
        "            noise = np.random.normal(0, 0.02, augmented.shape)\n",
        "            augmented = augmented + noise\n",
        "\n",
        "        if np.random.random() < 0.3:\n",
        "            angle = np.radians(np.random.uniform(-10, 10))\n",
        "            cos_a, sin_a = np.cos(angle), np.sin(angle)\n",
        "            for i in range(0, CONFIG['input_dim'], 2):\n",
        "                if i + 1 < CONFIG['input_dim']:\n",
        "                    x = augmented[:, i] - 0.5\n",
        "                    y = augmented[:, i + 1] - 0.5\n",
        "                    augmented[:, i] = cos_a * x - sin_a * y + 0.5\n",
        "                    augmented[:, i + 1] = sin_a * x + cos_a * y + 0.5\n",
        "\n",
        "        return augmented\n",
        "\n",
        "print(\"‚úÖ BdSLDataset class updated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "normalized_dir = Path(CONFIG['normalized_dir'])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = BdSLDataset(\n",
        "    train_samples,\n",
        "    word_to_label,\n",
        "    normalized_dir,\n",
        "    max_seq_length=CONFIG['seq_length'],\n",
        "    augment=CONFIG['augmentation'],\n",
        "    mode='train'\n",
        ")\n",
        "\n",
        "val_dataset = BdSLDataset(\n",
        "    val_samples,\n",
        "    word_to_label,\n",
        "    normalized_dir,\n",
        "    max_seq_length=CONFIG['seq_length'],\n",
        "    augment=False,\n",
        "    mode='val'\n",
        ")\n",
        "\n",
        "test_dataset = BdSLDataset(\n",
        "    test_samples,\n",
        "    word_to_label,\n",
        "    normalized_dir,\n",
        "    max_seq_length=CONFIG['seq_length'],\n",
        "    augment=False,\n",
        "    mode='test'\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['batch_size'] * 2,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CONFIG['batch_size'] * 2,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ DataLoaders created:\")\n",
        "print(f\"   Train: {len(train_loader)} batches\")\n",
        "print(f\"   Val: {len(val_loader)} batches\")\n",
        "print(f\"   Test: {len(test_loader)} batches\")\n",
        "print(f\"   Total train samples: {len(train_dataset)}\")\n",
        "print(f\"   Total val samples: {len(val_dataset)}\")\n",
        "print(f\"   Total test samples: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test data loading\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"‚úÖ Sample batch loaded successfully\")\n",
        "print(f\"   pose_sequence shape: {sample_batch['pose_sequence'].shape}\")\n",
        "print(f\"   label shape: {sample_batch['label'].shape}\")\n",
        "print(f\"   attention_mask shape: {sample_batch['attention_mask'].shape}\")\n",
        "print(f\"   seq_length shape: {sample_batch['seq_length'].shape}\")\n",
        "print(f\"\\n   Sample labels: {sample_batch['label'][:5].flatten().tolist()}\")\n",
        "print(f\"   Sample words: {[sample_batch['word'][i] for i in range(5)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=150):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_seq_length + 1, d_model)\n",
        "        position = torch.arange(0, max_seq_length + 1, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
        "                           (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        \n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Add positional encoding to input\"\"\"\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        # pe shape: (1, max_seq_len + 1, d_model)\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "print(\"‚úÖ PositionalEncoding class implemented\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 10: ProductionSPOTER Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProductionSPOTER(nn.Module):\n",
        "    def __init__(self, input_dim=108, d_model=108, num_heads=9,\n",
        "                 num_encoder_layers=8, d_ff=512, num_classes=72,\n",
        "                 max_seq_length=150, dropout=0.15):\n",
        "        super(ProductionSPOTER, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.max_seq_length = max_seq_length\n",
        "        \n",
        "        # Input projection\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Dropout(dropout * 0.5)\n",
        "        )\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "        \n",
        "        # Class token\n",
        "        self.class_token = nn.Parameter(torch.randn(1, 1, d_model) * 0.1)\n",
        "        \n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=d_ff,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout * 0.5),\n",
        "            nn.Linear(d_model // 2, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        Args:\n",
        "            x: (batch_size, seq_len, input_dim)\n",
        "            attention_mask: (batch_size, seq_len) - 1 for real, 0 for padding\n",
        "        Returns:\n",
        "            logits: (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # Project input\n",
        "        x = self.input_projection(x)\n",
        "        \n",
        "        # Add class token\n",
        "        class_tokens = self.class_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat([class_tokens, x], dim=1)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = self.positional_encoding(x)\n",
        "        \n",
        "        # Prepare attention mask for transformer\n",
        "        if attention_mask is not None:\n",
        "            # Add mask for class token (always attend)\n",
        "            class_mask = torch.ones(batch_size, 1, device=attention_mask.device)\n",
        "            full_mask = torch.cat([class_mask, attention_mask], dim=1)\n",
        "            # Convert to transformer format (True = masked, False = attend)\n",
        "            transformer_mask = (full_mask == 0)\n",
        "        else:\n",
        "            transformer_mask = None\n",
        "        \n",
        "        # Transformer encoder\n",
        "        encoded = self.transformer_encoder(x, src_key_padding_mask=transformer_mask)\n",
        "        \n",
        "        # Use class token for classification\n",
        "        class_representation = encoded[:, 0]\n",
        "        \n",
        "        # Classification\n",
        "        logits = self.classifier(class_representation)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "print(\"‚úÖ ProductionSPOTER model implemented\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 11: Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ProductionSPOTER(\n",
        "    input_dim=CONFIG['input_dim'],\n",
        "    d_model=CONFIG['d_model'],\n",
        "    num_heads=CONFIG['num_heads'],\n",
        "    num_encoder_layers=CONFIG['num_encoder_layers'],\n",
        "    d_ff=CONFIG['d_ff'],\n",
        "    num_classes=CONFIG['num_classes'],\n",
        "    max_seq_length=CONFIG['seq_length'],\n",
        "    dropout=CONFIG['dropout']\n",
        ").to(device)\n",
        "\n",
        "# Print model summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"‚úÖ Model initialized successfully\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Model size: {total_params * 4 / 1024**2:.2f} MB (float32)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test forward pass\n",
        "test_batch_size = 4\n",
        "test_input = torch.randn(test_batch_size, CONFIG['seq_length'], CONFIG['input_dim']).to(device)\n",
        "test_mask = torch.ones(test_batch_size, CONFIG['seq_length']).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(test_input, test_mask)\n",
        "\n",
        "print(f\"‚úÖ Forward pass test successful\")\n",
        "print(f\"   Input shape: {test_input.shape}\")\n",
        "print(f\"   Output shape: {logits.shape}\")\n",
        "print(f\"   Expected output shape: ({test_batch_size}, {CONFIG['num_classes']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 12: WandB Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# WandB initialization\n",
        "wandb.init(\n",
        "    project=\"bangla-sign-language-recognition\",\n",
        "    entity=None,  # Auto-detect\n",
        "    config=CONFIG,\n",
        "    name=f\"BdSL_SPOTER_{len(train_samples)}samples_{CONFIG['epochs']}epochs_{CONFIG['batch_size']}batch\"\n",
        ")\n",
        "\n",
        "# Watch model for gradients and parameters\n",
        "wandb.watch(model, log_freq=100)\n",
        "\n",
        "print(\"‚úÖ WandB initialized\")\n",
        "print(f\"   Project: bangla-sign-language-recognition\")\n",
        "print(f\"   Run name: BdSL_SPOTER_{len(train_samples)}samples_{CONFIG['epochs']}epochs_{CONFIG['batch_size']}batch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 13: Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        "    betas=(0.9, 0.95),\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "total_steps = len(train_loader) * CONFIG['epochs']\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=CONFIG['learning_rate'] * 3,  # Peak LR = 3x base\n",
        "    epochs=CONFIG['epochs'],\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.1,  # 10% warmup\n",
        "    anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "# Loss function with label smoothing\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
        "\n",
        "print(\"‚úÖ Training utilities configured\")\n",
        "print(f\"   Optimizer: AdamW (lr={CONFIG['learning_rate']}, weight_decay={CONFIG['weight_decay']})\")\n",
        "print(f\"   Scheduler: OneCycleLR (max_lr={CONFIG['learning_rate'] * 3:.6f})\")\n",
        "print(f\"   Loss: CrossEntropy (label_smoothing={CONFIG['label_smoothing']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device):\n",
        "    \"\"\"\n",
        "    Train for one epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        pose_sequences = batch['pose_sequence'].to(device)\n",
        "        labels = batch['label'].squeeze().to(device)\n",
        "        attention_masks = batch['attention_mask'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        logits = model(pose_sequences, attention_masks)\n",
        "        loss = criterion(logits, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Metrics\n",
        "        total_loss += loss.item() * len(labels)\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        total_correct += (predictions == labels).sum().item()\n",
        "        total_samples += len(labels)\n",
        "    \n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"\n",
        "    Validate the model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
        "            pose_sequences = batch['pose_sequence'].to(device)\n",
        "            labels = batch['label'].squeeze().to(device)\n",
        "            attention_masks = batch['attention_mask'].to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            logits = model(pose_sequences, attention_masks)\n",
        "            loss = criterion(logits, labels)\n",
        "            \n",
        "            # Metrics\n",
        "            total_loss += loss.item() * len(labels)\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            total_correct += (predictions == labels).sum().item()\n",
        "            total_samples += len(labels)\n",
        "    \n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "print(\"‚úÖ Training/validation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 14: Training Loop (50 Epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_val_acc = 0.0\n",
        "no_improve_count = 0\n",
        "patience = CONFIG['early_stopping_patience']\n",
        "history = {\n",
        "    'train_loss': [], \n",
        "    'train_acc': [], \n",
        "    'val_loss': [], \n",
        "    'val_acc': [],\n",
        "    'learning_rate': []\n",
        "}\n",
        "\n",
        "print(f\"üöÄ Starting training for {CONFIG['epochs']} epochs\")\n",
        "print(f\"   Early stopping patience: {patience}\")\n",
        "print(f\"   Primary metric: {CONFIG['primary_metric']}\")\n",
        "print(f\"   Checkpoint directory: {checkpoint_dir}\")\n",
        "\n",
        "for epoch in range(CONFIG['epochs']):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Epoch {epoch + 1}/{CONFIG['epochs']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Get current learning rate\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    \n",
        "    # Log to WandB\n",
        "    wandb.log({\n",
        "        'epoch': epoch + 1,\n",
        "        'train/loss': train_loss,\n",
        "        'train/accuracy': train_acc,\n",
        "        'val/loss': val_loss,\n",
        "        'val/accuracy': val_acc,\n",
        "        'learning_rate': current_lr\n",
        "    })\n",
        "    \n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['learning_rate'].append(current_lr)\n",
        "    \n",
        "    # Print summary\n",
        "    print(f\"\\nüìä Epoch {epoch + 1} Summary:\")\n",
        "    print(f\"   Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
        "    print(f\"   Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "    print(f\"   LR: {current_lr:.6f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        no_improve_count = 0\n",
        "        \n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'val_accuracy': val_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'config': CONFIG\n",
        "        }, checkpoint_dir / 'best_model.pth')\n",
        "        \n",
        "        print(f\"   ‚ú® New best model saved! Val Acc: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "        wandb.save(str(checkpoint_dir / 'best_model.pth'))\n",
        "    else:\n",
        "        no_improve_count += 1\n",
        "        print(f\"   No improvement for {no_improve_count} epochs\")\n",
        "    \n",
        "    # Early stopping check\n",
        "    if no_improve_count >= patience:\n",
        "        print(f\"\\n‚èπÔ∏è  Early stopping triggered after {patience} epochs without improvement\")\n",
        "        print(f\"   Best validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
        "        break\n",
        "\n",
        "print(f\"\\n‚úÖ Training complete!\")\n",
        "print(f\"   Total epochs trained: {epoch + 1}\")\n",
        "print(f\"   Best validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 15: Load Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best checkpoint\n",
        "checkpoint = torch.load(checkpoint_dir / 'best_model.pth', map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch']}\")\n",
        "print(f\"   Validation accuracy: {checkpoint['val_accuracy']:.4f} ({checkpoint['val_accuracy']*100:.2f}%)\")\n",
        "print(f\"   Validation loss: {checkpoint['val_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 16: Test Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "all_words = []\n",
        "all_signers = []\n",
        "all_probs = []\n",
        "\n",
        "print(\"üîç Evaluating on test set...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        pose_sequences = batch['pose_sequence'].to(device)\n",
        "        labels = batch['label'].squeeze().to(device)\n",
        "        words = batch['word']\n",
        "        signers = batch['signer']\n",
        "        \n",
        "        # Forward pass\n",
        "        logits = model(pose_sequences)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        \n",
        "        # Collect results\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_words.extend(words)\n",
        "        all_signers.extend(signers)\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "print(f\"‚úÖ Test evaluation complete\")\n",
        "print(f\"   Total test samples: {len(all_predictions)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate overall metrics\n",
        "test_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "test_precision = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "test_recall = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "test_f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "\n",
        "print(f\"\\nüìä Test Results:\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Top-1 Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"Precision (macro): {test_precision:.4f} ({test_precision*100:.2f}%)\")\n",
        "print(f\"Recall (macro): {test_recall:.4f} ({test_recall*100:.2f}%)\")\n",
        "print(f\"F1-Score (macro): {test_f1:.4f} ({test_f1*100:.2f}%)\")\n",
        "\n",
        "# Log to WandB\n",
        "wandb.log({\n",
        "    'test/accuracy': test_accuracy,\n",
        "    'test/precision': test_precision,\n",
        "    'test/recall': test_recall,\n",
        "    'test/f1_score': test_f1\n",
        "})\n",
        "\n",
        "print(f\"\\n‚úÖ Metrics logged to WandB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 17: Per-Signer Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate accuracy per signer\n",
        "signer_accuracies = {}\n",
        "signer_counts = {}\n",
        "\n",
        "print(f\"\\nüìä Per-Signer Analysis:\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "for signer in ['S01', 'S02', 'S05']:\n",
        "    signer_indices = [i for i, s in enumerate(all_signers) if s == signer]\n",
        "    if signer_indices:\n",
        "        signer_labels = [all_labels[i] for i in signer_indices]\n",
        "        signer_preds = [all_predictions[i] for i in signer_indices]\n",
        "        acc = accuracy_score(signer_labels, signer_preds)\n",
        "        signer_accuracies[signer] = acc\n",
        "        signer_counts[signer] = len(signer_indices)\n",
        "        print(f\"{signer}: {acc:.4f} ({acc*100:.2f}%) - {len(signer_indices)} samples\")\n",
        "\n",
        "# Log to WandB\n",
        "wandb.log({'test/signer_accuracies': signer_accuracies})\n",
        "\n",
        "print(f\"\\n‚úÖ Per-signer accuracies logged to WandB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize per-signer accuracy\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "signers = list(signer_accuracies.keys())\n",
        "accuracies = [signer_accuracies[s] * 100 for s in signers]\n",
        "counts = [signer_counts[s] for s in signers]\n",
        "\n",
        "bars = ax.bar(signers, accuracies, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.8)\n",
        "ax.set_xlabel('Signer', fontsize=12)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Per-Signer Test Accuracy', fontsize=14, fontweight='bold')\n",
        "ax.set_ylim(0, 100)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add sample count labels on bars\n",
        "for bar, count in zip(bars, counts):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{int(height)}%\\n(n={count})',\n",
        "            ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(checkpoint_dir / 'per_signer_accuracy.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Log to WandB\n",
        "wandb.log({'test/per_signer_accuracy_plot': wandb.Image(str(checkpoint_dir / 'per_signer_accuracy.png'))})\n",
        "\n",
        "print(f\"‚úÖ Per-signer accuracy plot saved and logged to WandB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 18: Confusion Matrix & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "# Normalize by row (true labels)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "print(f\"‚úÖ Confusion matrix created\")\n",
        "print(f\"   Shape: {cm.shape}\")\n",
        "print(f\"   Total samples: {cm.sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrix (full 72x72)\n",
        "plt.style.use('default')\n",
        "fig, ax = plt.subplots(figsize=(24, 20))\n",
        "\n",
        "# Create heatmap with short labels\n",
        "short_labels = [label_to_word[i][:8] + '..' if len(label_to_word[i]) > 10 else label_to_word[i] \n",
        "               for i in range(72)]\n",
        "\n",
        "sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', \n",
        "            xticklabels=short_labels,\n",
        "            yticklabels=short_labels,\n",
        "            ax=ax, cbar_kws={'label': 'Count'})\n",
        "\n",
        "ax.set_xlabel('Predicted Label', fontsize=12)\n",
        "ax.set_ylabel('True Label', fontsize=12)\n",
        "ax.set_title('Confusion Matrix (72 Classes)', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xticks(rotation=90, fontsize=8)\n",
        "plt.yticks(rotation=0, fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(checkpoint_dir / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Log to WandB\n",
        "wandb.log({'test/confusion_matrix': wandb.Image(str(checkpoint_dir / 'confusion_matrix.png'))})\n",
        "\n",
        "print(f\"‚úÖ Confusion matrix saved and logged to WandB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot normalized confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(24, 20))\n",
        "\n",
        "sns.heatmap(cm_normalized, annot=False, fmt='.2f', cmap='Blues', \n",
        "            xticklabels=short_labels,\n",
        "            yticklabels=short_labels,\n",
        "            ax=ax, cbar_kws={'label': 'Normalized Count'},\n",
        "            vmin=0, vmax=1)\n",
        "\n",
        "ax.set_xlabel('Predicted Label', fontsize=12)\n",
        "ax.set_ylabel('True Label', fontsize=12)\n",
        "ax.set_title('Normalized Confusion Matrix (72 Classes)', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xticks(rotation=90, fontsize=8)\n",
        "plt.yticks(rotation=0, fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(checkpoint_dir / 'confusion_matrix_normalized.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Log to WandB\n",
        "wandb.log({'test/confusion_matrix_normalized': wandb.Image(str(checkpoint_dir / 'confusion_matrix_normalized.png'))})\n",
        "\n",
        "print(f\"‚úÖ Normalized confusion matrix saved and logged to WandB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 19: Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Loss curves\n",
        "axes[0, 0].plot(history['train_loss'], label='Train', linewidth=2)\n",
        "axes[0, 0].plot(history['val_loss'], label='Validation', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Loss', fontsize=11)\n",
        "axes[0, 0].set_title('Training Loss', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "axes[0, 1].plot(history['train_acc'], label='Train', linewidth=2)\n",
        "axes[0, 1].plot(history['val_acc'], label='Validation', linewidth=2)\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Accuracy', fontsize=11)\n",
        "axes[0, 1].set_title('Training Accuracy', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=10)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate curve\n",
        "axes[1, 0].plot(history['learning_rate'], linewidth=2, color='orange')\n",
        "axes[1, 0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Learning Rate', fontsize=11)\n",
        "axes[1, 0].set_title('Learning Rate Schedule', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Combined plot\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(history['train_acc'], label='Train Acc', linewidth=2, color='blue')\n",
        "ax4.plot(history['val_acc'], label='Val Acc', linewidth=2, color='red')\n",
        "ax4.set_xlabel('Epoch', fontsize=11)\n",
        "ax4.set_ylabel('Accuracy', fontsize=11)\n",
        "ax4.set_title('Accuracy Comparison', fontsize=13, fontweight='bold')\n",
        "ax4.legend(fontsize=10)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(checkpoint_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Log to WandB\n",
        "wandb.log({'training_curves': wandb.Image(str(checkpoint_dir / 'training_curves.png'))})\n",
        "\n",
        "print(f\"‚úÖ Training curves saved and logged to WandB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 20: Top-5 Accuracy Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate top-5 accuracy\n",
        "def top_k_accuracy(labels, probs, k=5):\n",
        "    \"\"\"\n",
        "    Calculate top-k accuracy\n",
        "    Args:\n",
        "        labels: true labels (batch_size,)\n",
        "        probs: predicted probabilities (batch_size, num_classes)\n",
        "        k: top-k to consider\n",
        "    \"\"\"\n",
        "    top_k_preds = np.argsort(probs, axis=1)[:, -k:]\n",
        "    correct = 0\n",
        "    for i, label in enumerate(labels):\n",
        "        if label in top_k_preds[i]:\n",
        "            correct += 1\n",
        "    return correct / len(labels)\n",
        "\n",
        "# Calculate top-5 accuracy\n",
        "all_probs_array = np.array(all_probs)\n",
        "top5_acc = top_k_accuracy(all_labels, all_probs_array, k=5)\n",
        "\n",
        "print(f\"\\nüìä Top-K Accuracy:\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Top-1 Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"Top-5 Accuracy: {top5_acc:.4f} ({top5_acc*100:.2f}%)\")\n",
        "\n",
        "# Log to WandB\n",
        "wandb.log({\n",
        "    'test/top5_accuracy': top5_acc\n",
        "})\n",
        "\n",
        "print(f\"\\n‚úÖ Top-5 accuracy logged to WandB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 21: Per-Class Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate per-class accuracy\n",
        "from collections import defaultdict\n",
        "\n",
        "class_correct = defaultdict(int)\n",
        "class_total = defaultdict(int)\n",
        "\n",
        "for true_label, pred_label in zip(all_labels, all_predictions):\n",
        "    class_total[true_label] += 1\n",
        "    if true_label == pred_label:\n",
        "        class_correct[true_label] += 1\n",
        "\n",
        "per_class_accuracy = {}\n",
        "for label_id in range(CONFIG['num_classes']):\n",
        "    if class_total[label_id] > 0:\n",
        "        per_class_accuracy[label_id] = class_correct[label_id] / class_total[label_id]\n",
        "    else:\n",
        "        per_class_accuracy[label_id] = 0.0\n",
        "\n",
        "# Create DataFrame for easy analysis\n",
        "class_df = pd.DataFrame([\n",
        "    {'label_id': i, 'word': label_to_word[i], 'accuracy': per_class_accuracy[i], \n",
        "     'correct': class_correct[i], 'total': class_total[i]}\n",
        "    for i in range(CONFIG['num_classes'])\n",
        "])\n",
        "\n",
        "# Sort by accuracy\n",
        "class_df = class_df.sort_values('accuracy')\n",
        "\n",
        "# Show worst and best performing classes\n",
        "print(f\"\\nüìä Per-Class Analysis:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\nWorst 5 performing classes:\")\n",
        "print(class_df.head(5).to_string(index=False))\n",
        "\n",
        "print(f\"\\nBest 5 performing classes:\")\n",
        "print(class_df.tail(5)[::-1].to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot per-class accuracy distribution\n",
        "fig, ax = plt.subplots(figsize=(15, 6))\n",
        "\n",
        "ax.bar(range(CONFIG['num_classes']), \n",
        "       [per_class_accuracy[i] * 100 for i in range(CONFIG['num_classes'])],\n",
        "       color='steelblue', alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Class ID', fontsize=11)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=11)\n",
        "ax.set_title('Per-Class Accuracy (72 Classes)', fontsize=14, fontweight='bold')\n",
        "ax.set_ylim(0, 100)\n",
        "ax.axhline(y=test_accuracy*100, color='red', linestyle='--', linewidth=2, label=f'Overall Accuracy: {test_accuracy*100:.1f}%')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "ax.legend(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(checkpoint_dir / 'per_class_accuracy.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Log to WandB\n",
        "wandb.log({'test/per_class_accuracy_plot': wandb.Image(str(checkpoint_dir / 'per_class_accuracy.png'))})\n",
        "\n",
        "print(f\"‚úÖ Per-class accuracy plot saved and logged to WandB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 22: Save Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive test results\n",
        "results = {\n",
        "    'test_accuracy': float(test_accuracy),\n",
        "    'test_precision': float(test_precision),\n",
        "    'test_recall': float(test_recall),\n",
        "    'test_f1_score': float(test_f1),\n",
        "    'test_top5_accuracy': float(top5_acc),\n",
        "    'signer_accuracies': {k: float(v) for k, v in signer_accuracies.items()},\n",
        "    'signer_counts': {k: int(v) for k, v in signer_counts.items()},\n",
        "    'best_val_accuracy': float(best_val_acc),\n",
        "    'best_epoch': int(checkpoint['epoch']),\n",
        "    'total_epochs_trained': int(checkpoint['epoch']),\n",
        "    'model_parameters': int(total_params),\n",
        "    'config': CONFIG,\n",
        "    'per_class_accuracy': {i: float(per_class_accuracy[i]) for i in range(CONFIG['num_classes'])}\n",
        "}\n",
        "\n",
        "with open(checkpoint_dir / 'test_results.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Test results saved to {checkpoint_dir / 'test_results.json'}\")\n",
        "\n",
        "# Also save as CSV for easy viewing\n",
        "results_df = pd.DataFrame([\n",
        "    {'Metric': 'Test Accuracy', 'Value': f\"{test_accuracy:.4f} ({test_accuracy*100:.2f}%)\"},\n",
        "    {'Metric': 'Test Precision (macro)', 'Value': f\"{test_precision:.4f} ({test_precision*100:.2f}%)\"},\n",
        "    {'Metric': 'Test Recall (macro)', 'Value': f\"{test_recall:.4f} ({test_recall*100:.2f}%)\"},\n",
        "    {'Metric': 'Test F1-Score (macro)', 'Value': f\"{test_f1:.4f} ({test_f1*100:.2f}%)\"},\n",
        "    {'Metric': 'Top-5 Accuracy', 'Value': f\"{top5_acc:.4f} ({top5_acc*100:.2f}%)\"},\n",
        "    {'Metric': 'Best Val Accuracy', 'Value': f\"{best_val_acc:.4f} ({best_val_acc*100:.2f}%)\"},\n",
        "    {'Metric': 'Best Epoch', 'Value': str(checkpoint['epoch'])},\n",
        "    {'Metric': 'Total Parameters', 'Value': f\"{total_params:,}\"},\n",
        "    {'Metric': 'Model Size (MB)', 'Value': f\"{total_params * 4 / 1024**2:.2f}\"},\n",
        "])\n",
        "\n",
        "results_df.to_csv(checkpoint_dir / 'test_results_summary.csv', index=False)\n",
        "print(f\"‚úÖ Test results summary saved to {checkpoint_dir / 'test_results_summary.csv'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model\n",
        "torch.save(model.state_dict(), checkpoint_dir / 'final_model.pth')\n",
        "wandb.save(str(checkpoint_dir / 'final_model.pth'))\n",
        "\n",
        "# Save model architecture summary\n",
        "model_summary = {\n",
        "    'architecture': 'ProductionSPOTER',\n",
        "    'input_dim': CONFIG['input_dim'],\n",
        "    'd_model': CONFIG['d_model'],\n",
        "    'num_heads': CONFIG['num_heads'],\n",
        "    'num_encoder_layers': CONFIG['num_encoder_layers'],\n",
        "    'd_ff': CONFIG['d_ff'],\n",
        "    'num_classes': CONFIG['num_classes'],\n",
        "    'max_seq_length': CONFIG['seq_length'],\n",
        "    'dropout': CONFIG['dropout'],\n",
        "    'total_parameters': int(total_params),\n",
        "    'trainable_parameters': int(trainable_params)\n",
        "}\n",
        "\n",
        "with open(checkpoint_dir / 'model_summary.json', 'w') as f:\n",
        "    json.dump(model_summary, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Final model saved to {checkpoint_dir / 'final_model.pth'}\")\n",
        "print(f\"‚úÖ Model summary saved to {checkpoint_dir / 'model_summary.json'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23: WandB Finalization & Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Log final artifacts to WandB\n",
        "artifacts = [\n",
        "    str(checkpoint_dir / 'best_model.pth'),\n",
        "    str(checkpoint_dir / 'final_model.pth'),\n",
        "    str(checkpoint_dir / 'label_mapping.json'),\n",
        "    str(checkpoint_dir / 'test_results.json'),\n",
        "    str(checkpoint_dir / 'model_summary.json'),\n",
        "]\n",
        "\n",
        "for artifact_path in artifacts:\n",
        "    wandb.save(artifact_path)\n",
        "    print(f\"‚úÖ Uploaded to WandB: {Path(artifact_path).name}\")\n",
        "\n",
        "# Finish WandB run\n",
        "wandb.finish()\n",
        "print(f\"\\n‚úÖ WandB run completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"üéâ TRAINING COMPLETE - FINAL SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nüìÅ Output Directory: {checkpoint_dir}\")\n",
        "print(f\"\\nüìä Test Results:\")\n",
        "print(f\"   Top-1 Accuracy:     {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"   Top-5 Accuracy:     {top5_acc:.4f} ({top5_acc*100:.2f}%)\")\n",
        "print(f\"   Precision (macro):  {test_precision:.4f} ({test_precision*100:.2f}%)\")\n",
        "print(f\"   Recall (macro):     {test_recall:.4f} ({test_recall*100:.2f}%)\")\n",
        "print(f\"   F1-Score (macro):   {test_f1:.4f} ({test_f1*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nüë• Per-Signer Performance:\")\n",
        "for signer, acc in signer_accuracies.items():\n",
        "    print(f\"   {signer}: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nüèãÔ∏è  Model Information:\")\n",
        "print(f\"   Architecture:      ProductionSPOTER\")\n",
        "print(f\"   Total Parameters: {total_params:,}\")\n",
        "print(f\"   Model Size:       {total_params * 4 / 1024**2:.2f} MB\")\n",
        "print(f\"   Best Val Acc:     {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
        "print(f\"   Best Epoch:       {checkpoint['epoch']}\")\n",
        "\n",
        "print(f\"\\nüì¶ Generated Files:\")\n",
        "output_files = [\n",
        "    'best_model.pth',\n",
        "    'final_model.pth',\n",
        "    'label_mapping.json',\n",
        "    'test_results.json',\n",
        "    'test_results_summary.csv',\n",
        "    'model_summary.json',\n",
        "    'confusion_matrix.png',\n",
        "    'confusion_matrix_normalized.png',\n",
        "    'training_curves.png',\n",
        "    'per_signer_accuracy.png',\n",
        "    'per_class_accuracy.png',\n",
        "    'signer_distribution.png',\n",
        "    'grammar_distribution.png'\n",
        "]\n",
        "\n",
        "for file_name in output_files:\n",
        "    file_path = checkpoint_dir / file_name\n",
        "    if file_path.exists():\n",
        "        file_size = file_path.stat().st_size\n",
        "        size_mb = file_size / (1024 * 1024)\n",
        "        print(f\"   ‚úì {file_name:45s} ({size_mb:6.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"   ‚úó {file_name:45s} (missing)\")\n",
        "\n",
        "print(f\"\\nüåê WandB:\")\n",
        "print(f\"   Project: bangla-sign-language-recognition\")\n",
        "print(f\"   All metrics and artifacts logged\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"üöÄ Ready for inference and deployment!\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix: Inference Function Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inference(model, input_data, label_to_word, device, top_k=5):\n",
        "    \"\"\"\n",
        "    Run inference on a single sample\n",
        "    \n",
        "    Args:\n",
        "        model: Trained ProductionSPOTER model\n",
        "        input_data: Pose sequence (seq_length, 108) or path to .npz file\n",
        "        label_to_word: Label to word mapping dictionary\n",
        "        device: Device (cuda/cpu)\n",
        "        top_k: Number of top predictions to return\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with predictions and confidences\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Load input data\n",
        "    if isinstance(input_data, (str, Path)):\n",
        "        # Load from .npz file\n",
        "        data = np.load(input_data)\n",
        "        pose_sequence = data[list(data.keys())[0]]\n",
        "    else:\n",
        "        pose_sequence = np.array(input_data)\n",
        "    \n",
        "    # Ensure correct shape\n",
        "    if len(pose_sequence.shape) != 2:\n",
        "        raise ValueError(f\"Expected shape (seq_len, 108), got {pose_sequence.shape}\")\n",
        "    \n",
        "    # Pad or truncate to 150 frames\n",
        "    if pose_sequence.shape[0] > 150:\n",
        "        pose_sequence = pose_sequence[:150]\n",
        "    elif pose_sequence.shape[0] < 150:\n",
        "        padding = np.zeros((150 - pose_sequence.shape[0], 108))\n",
        "        pose_sequence = np.vstack([pose_sequence, padding])\n",
        "    \n",
        "    # Create attention mask\n",
        "    seq_length = pose_sequence.shape[0]\n",
        "    attention_mask = torch.ones(150)\n",
        "    attention_mask[seq_length:] = 0\n",
        "    \n",
        "    # Convert to tensor and move to device\n",
        "    pose_tensor = torch.FloatTensor(pose_sequence).unsqueeze(0).to(device)  # Add batch dimension\n",
        "    mask_tensor = torch.FloatTensor(attention_mask).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(pose_tensor, mask_tensor)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        \n",
        "        # Get top-k predictions\n",
        "        top_k_probs, top_k_indices = torch.topk(probs, k=top_k)\n",
        "    \n",
        "    # Prepare results\n",
        "    results = {\n",
        "        'predicted_word': label_to_word[top_k_indices[0][0].item()],\n",
        "        'confidence': top_k_probs[0][0].item(),\n",
        "        'top_k_predictions': [\n",
        "            {\n",
        "                'word': label_to_word[idx.item()],\n",
        "                'confidence': prob.item()\n",
        "            }\n",
        "            for idx, prob in zip(top_k_indices[0], top_k_probs[0])\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Inference function template created\")\n",
        "print(f\"\\nUsage example:\")\n",
        "print(f\"  results = inference(model, 'path/to/sample.npz', label_to_word, device)\")\n",
        "print(f\"  print(f'Predicted: {results[\"predicted_word\"]}')\")\n",
        "print(f\"  print(f'Confidence: {results[\"confidence\"]:.2%}')\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}